---
title: "Star Wars (4-6) Text Analysis of Movie Scripts"
output:
  pdf_document:
    toc: true
    number_sections: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

# Dependencies

```{r}
library(tidyverse)
library(tidytext)
```

# Reading in clean data

```{r}
sw_scripts <- read_csv("clean_data/original_sw_trilogy.csv")
```

# Tokenize and remove stop words

```{r}
sw_tokens <- sw_scripts %>%
  unnest_tokens(
    word,
    dialogue
  ) %>%
  anti_join(stop_words)

sw_tokens

```

# Check which sentiment lexicon categorizes most words

```{r}
unique_words <- sw_tokens %>% distinct(word)

# vector of available lexicons in tidytext::get_sentiments()
lexicons <- c("bing", "afinn", "loughran", "nrc")

# create list of joined datasets with available lexicons
nested_df <- lexicons %>%
  map(~left_join(unique_words, get_sentiments(.), by = "word"))

# attach lexicon names to list
names(nested_df) <- lexicons 


for (lexicon in lexicons){

  # 2nd element is sentiment category or rating
  sentiments <- nested_df[[lexicon]][[2]]
  
  # count all values without attached sentiment
  missing <- sum(is.na(sentiments)) 
  
  print(str_glue("{lexicon}: {missing} uncategorised words"))

}

```

